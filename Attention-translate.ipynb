{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T16:49:33.260563Z",
     "start_time": "2018-09-24T16:49:25.538338Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import fasttext as ft\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T19:54:42.221217Z",
     "start_time": "2018-09-24T19:54:42.219210Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'DATA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T20:04:34.256334Z",
     "start_time": "2018-09-24T20:04:33.571008Z"
    }
   },
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path + 'rus.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:-1]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t ' + target_text + ' \\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "num_samples = len(input_texts)\n",
    "vocab_size = 50000\n",
    "\n",
    "from itertools import chain\n",
    "max_len = max(list(chain.from_iterable((len(x.split(' ')), len(y.split(' '))) for x, y in zip(input_texts, target_texts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T20:04:46.587380Z",
     "start_time": "2018-09-24T20:04:46.028484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 304513\n",
      "Max sequence length for inputs: 43\n",
      "Max sequence length for outputs: 42\n",
      "Median sequence length for inputs: 6.0\n",
      "Median sequence length for outputs: 7.0\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', num_samples)\n",
    "print('Max sequence length for inputs:', max([len(txt.split(' ')) for txt in input_texts]))\n",
    "print('Max sequence length for outputs:', max([len(txt.split(' ')) for txt in target_texts]))\n",
    "print('Median sequence length for inputs:', np.median([len(txt.split(' ')) for txt in input_texts]))\n",
    "print('Median sequence length for outputs:', np.median([len(txt.split(' ')) for txt in target_texts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T20:08:19.743063Z",
     "start_time": "2018-09-24T20:08:02.834810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 1\n",
      "239 1\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = Tokenizer(num_words=vocab_size, lower=True, split=' ', oov_token='OOV')\n",
    "ru_tokenizer = Tokenizer(num_words=vocab_size, lower=True, split=' ', oov_token='OOV')\n",
    "en_tokenizer.fit_on_texts(input_texts)\n",
    "ru_tokenizer.fit_on_texts(target_texts)\n",
    "\n",
    "x_t = np.asarray(en_tokenizer.texts_to_sequences(input_texts))\n",
    "y_t = np.asarray(ru_tokenizer.texts_to_sequences(target_texts))\n",
    "print(en_tokenizer.word_index['coffee'], en_tokenizer.word_index['OOV'])\n",
    "print(ru_tokenizer.word_index['кофе'], ru_tokenizer.word_index['OOV'])\n",
    "\n",
    "x_t = pad_sequences(x_t, maxlen=max_len, dtype='int32', padding='post', truncating='post', value=0)\n",
    "y_t = pad_sequences(y_t, maxlen=max_len, dtype='int32', padding='post', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T20:09:30.157648Z",
     "start_time": "2018-09-24T20:09:30.154638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Go.', '\\t Иди. \\n')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[0], target_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T20:15:48.326293Z",
     "start_time": "2018-09-24T20:15:48.322280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([37,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " array([722,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]),\n",
       " (304513, 43))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t[0], y_t[0], x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T20:27:40.265818Z",
     "start_time": "2018-09-24T20:27:40.193573Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, inp_sz, out_sz, em_sz, h_sz, n_l, voc_sz):\n",
    "        super().__init__()\n",
    "        self.em_sz, self.h_sz, self.n_l, self.inp_sz, self.out_sz, self.voc_sz = em_sz, h_sz, n_l, inp_sz, out_sz, voc_sz\n",
    "        # Encoder\n",
    "        self.enc_em = nn.Embedding(self.voc_sz, self.em_sz)\n",
    "        self.em_drp = nn.Dropout(0.15)\n",
    "        self.enc_gru = nn.GRU(self.em_sz, self.h_sz, num_layers=self.n_l, dropout=0.2)\n",
    "        self.enc_drp = nn.Dropout(0.3)\n",
    "        self.dec_out = nn.Linear(self.h_sz, self.em_sz, bias=False)\n",
    "        # Decoder\n",
    "        self.dec_em = nn.Embedding(self.voc_sz, self.em_sz)\n",
    "        self.dec_gru = nn.GRU(self.em_sz, self.h_sz, num_layers=self.n_l, dropout=0.2)\n",
    "        self.dec_drp = nn.Dropout(0.3)\n",
    "        self.dec_out = nn.Linear(self.em_sz, self.out_sz)\n",
    "        #self.out.weight.data = self.enc_em.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        sl, bs = inp.shape\n",
    "        h = self.initHidden(bs)\n",
    "        \n",
    "        x = self.em_drp(self.em_sz(inp))\n",
    "        enc_out, h = self.enc_drp(self.enc_em(x, h))\n",
    "        h = self.out_enc(h)\n",
    "        \n",
    "        dec_inp = torch.zeros(bs).long()\n",
    "        result = []\n",
    "        # What is this?\n",
    "        for i in range(self.out_sz):\n",
    "            emb = self.dec_emb(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.dec_gru(emb, h)\n",
    "            outp = self.out(self.dec_drp(outp[0]))\n",
    "            result.append(outp)\n",
    "            dec_inp = outp.data.max(1)[1]\n",
    "            if (dec_inp==1).all():\n",
    "                break\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): \n",
    "        # Num_layers, batch size, num hidden\n",
    "        return torch.zeros(self.nl, bs, self.nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T20:27:41.123006Z",
     "start_time": "2018-09-24T20:27:40.776832Z"
    }
   },
   "outputs": [],
   "source": [
    "em_sz = 300\n",
    "n_h = 128\n",
    "n_l = 2\n",
    "\n",
    "inp_sz = max_len\n",
    "model = AttentionRNN(inp_sz, vocab_size, em_sz, n_h, n_l, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:58:26.564284Z",
     "start_time": "2018-09-23T10:58:26.547745Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "###RUN THROUGH DL2 TRANSLATE NOTEBOOK AND ANSWER THESE QUESTIONS\n",
    "#########################################\n",
    "## WHAT IS \n",
    "\n",
    "#vecs_enc - \n",
    "    # Dict of words, with embedding vectors values\n",
    "    # https://i.imgur.com/nIELpdY.png\n",
    "    # https://i.imgur.com/RgBnu4O.png\n",
    "#itos_enc - \n",
    "    # Index to string\n",
    "    # List of strings, of which the list index is pointing to a word\n",
    "    # https://i.imgur.com/oq6Kcv1.png\n",
    "    # https://i.imgur.com/dXSh60V.png\n",
    "#vecs_dec - Same as vecs_enc, but for dec\n",
    "#itos_dec - Same as itos_enc but for dec\n",
    "#########################################\n",
    "##WHAT DOES create_emb DO\n",
    "    # Makes an embedding with wiki vectors weights tripled \n",
    "## WHAT IS THE sl,bs IN inp.size()\n",
    "    # bs is batch size\n",
    "    # sl is seq_len https://i.imgur.com/icfxqv9.png\n",
    "\n",
    "##WHAT DOES THE FOR LOOP IN FORWARD DO\n",
    "\n",
    "##WHY DO YOU TAKE WEIGHT DATA OF OUTPUT EMBEDDINGS (IS THAT RELATED TO THE RETURN?)\n",
    "\n",
    "\n",
    "##########################################\n",
    "###FIGURE OUT THE LOSS FUNCTION\n",
    "\n",
    "## WHY DO YOU PAD THE INPUT LIKE THAT\n",
    "\n",
    "## WHY DO YOU SLICE THE INPUT\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:46.624860Z",
     "start_time": "2018-09-22T11:58:46.622855Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRY TO TRAIN IT, WRITE OWN TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:25.249402Z",
     "start_time": "2018-09-22T11:58:25.247405Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONVERT IT TO BIDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:08.624959Z",
     "start_time": "2018-09-22T11:58:08.622955Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEACHER FORCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:15.176179Z",
     "start_time": "2018-09-22T11:58:15.174175Z"
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:34.922938Z",
     "start_time": "2018-09-22T11:58:34.920934Z"
    }
   },
   "outputs": [],
   "source": [
    "# ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:59:06.256912Z",
     "start_time": "2018-09-22T11:59:06.254906Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONVERT MODEL TO PRDOCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
