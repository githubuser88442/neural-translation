{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:03:56.600938Z",
     "start_time": "2018-09-23T10:03:54.727771Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:03:56.604450Z",
     "start_time": "2018-09-23T10:03:56.602444Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'DATA/rus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:04:00.547845Z",
     "start_time": "2018-09-23T10:03:58.718879Z"
    }
   },
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:-1]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:04:00.682192Z",
     "start_time": "2018-09-23T10:04:00.548848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 304513\n",
      "Number of unique input tokens: 93\n",
      "Number of unique output tokens: 158\n",
      "Max sequence length for inputs: 239\n",
      "Max sequence length for outputs: 267\n",
      "Median sequence length for inputs: 27.0\n",
      "Median sequence length for outputs: 29.0\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max([len(txt) for txt in input_texts]))\n",
    "print('Max sequence length for outputs:', max([len(txt) for txt in target_texts]))\n",
    "print('Median sequence length for inputs:', np.median([len(txt) for txt in input_texts]))\n",
    "print('Median sequence length for outputs:', np.median([len(txt) for txt in target_texts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:04:00.685199Z",
     "start_time": "2018-09-23T10:04:00.683193Z"
    }
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 45\n",
    "max_decoder_seq_length = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:04:08.367772Z",
     "start_time": "2018-09-23T10:04:00.686200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304513, 45)\n",
      "(304513, 45)\n",
      "(304513, 45)\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length),dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # Zip stops after it hits the max seq len\n",
    "    for t, char in zip(range(max_encoder_seq_length), input_text):\n",
    "        encoder_input_data[i, t] = input_token_index[char]\n",
    "    for t, char in zip(range(0, max_decoder_seq_length), target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[char]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1] = target_token_index[char]\n",
    "\n",
    "print(decoder_target_data.shape)\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:04:08.377304Z",
     "start_time": "2018-09-23T10:04:08.368774Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_emb(vecs, itos, em_sz):\n",
    "    # Make regular embedding with vocab sz, em_sz, and pad\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    # Get embedding weights\n",
    "    wgts = emb.weight.data\n",
    "    \n",
    "    miss = []\n",
    "    for i,w in enumerate(itos):\n",
    "        # Idk triple it or something\n",
    "        try: wgts[i] = torch.from_numpy(vecs[w]*3)\n",
    "        # If you missed some, append to w without tripling \n",
    "        except: miss.append(w)\n",
    "    print(len(miss),miss[5:10])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:04:08.465524Z",
     "start_time": "2018-09-23T10:04:08.378809Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    def initHidden(self, bs):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(vecs_enc, vecs_dec, em_sz, n_h, n_l, inp_sz, out_sz):\n",
    "        super().__init__()\n",
    "        self.em_sz, self.n_h, self.n_l, self.inp_sz, self.out_sz = em_sz, n_h, n_l, inp_sz, out_sz\n",
    "        # Encoder\n",
    "        self.enc_em = create_emb(self.inp_sz, itos_enc, self.em_sz)\n",
    "        self.em_drp = nn.Dropout(0.15)\n",
    "        self.enc_gru = nn.GRU(self.em_sz, self.h_sz, num_layers=self.n_l, dropout=0.2)\n",
    "        self.enc_drp = nn.Dropout(0.3)\n",
    "        self.dec_out = nn.Linear(self.h_sz, self.em_sz, bias=False)\n",
    "        # Decoder\n",
    "        self.dec_em = create_emb(self.h_sz, itos_dec, self.em_sz)\n",
    "        self.dec_gru = nn.GRU(self.em_sz, self.h_sz, num_layers=self.n_l, dropout=0.2)\n",
    "        self.dec_drp = nn.Dropout(0.3)\n",
    "        self.dec_out = nn.Linear(self.em_sz, self.out_sz)\n",
    "        self.out.weight.data = self.enc_em.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        sl, bs = inp.shape\n",
    "        h = self.initHidden(bs)\n",
    "        \n",
    "        x = self.em_drp(self.em_sz(inp))\n",
    "        enc_out, h = self.enc_drp(self.enc_em(x, h))\n",
    "        h = self.out_enc(h)\n",
    "        \n",
    "        dec_inp = torch.zeros(bs).long()\n",
    "        result = []\n",
    "        # What is this?\n",
    "        for i in range(self.out_sz):\n",
    "            emb = self.dec_emb(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.dec_gru(emb, h)\n",
    "            outp = self.out(self.dec_drp(outp[0]))\n",
    "            result.append(outp)\n",
    "            dec_inp = outp.data.max(1)[1]\n",
    "            if (dec_inp==1).all(): break\n",
    "        return torch.stack(res)\n",
    "    \n",
    "     def initHidden(self, bs): \n",
    "        # Num_layers, batch size, num hidden\n",
    "        return torch.zeros(self.nl, bs, self.nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:04:08.465524Z",
     "start_time": "2018-09-23T10:04:04.380Z"
    }
   },
   "outputs": [],
   "source": [
    "n_h = 128\n",
    "n_l = 2\n",
    "inp_sz = num_encoder_tokens max_encoder_seq_length\n",
    "out_sz = num_decoder_tokens\n",
    "model = AttentionRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-23T10:58:26.564284Z",
     "start_time": "2018-09-23T10:58:26.547745Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "###RUN THROUGH DL2 TRANSLATE NOTEBOOK AND ANSWER THESE QUESTIONS\n",
    "#########################################\n",
    "## WHAT IS \n",
    "\n",
    "#vecs_enc - \n",
    "    # Dict of words, with embedding vectors values\n",
    "    # https://i.imgur.com/nIELpdY.png\n",
    "    # https://i.imgur.com/RgBnu4O.png\n",
    "#itos_enc - \n",
    "    # Index to string\n",
    "    # List of strings, of which the list index is pointing to a word\n",
    "    # https://i.imgur.com/oq6Kcv1.png\n",
    "    # https://i.imgur.com/dXSh60V.png\n",
    "#vecs_dec - Same as vecs_enc, but for dec\n",
    "#itos_dec - Same as itos_enc but for dec\n",
    "#########################################\n",
    "##WHAT DOES create_emb DO\n",
    "    # Makes an embedding with wiki vectors weights tripled \n",
    "## WHAT IS THE sl,bs IN inp.size()\n",
    "    # bs is batch size\n",
    "    # sl is seq_len https://i.imgur.com/icfxqv9.png\n",
    "\n",
    "##WHAT DOES THE FOR LOOP IN FORWARD DO\n",
    "\n",
    "##WHY DO YOU TAKE WEIGHT DATA OF OUTPUT EMBEDDINGS (IS THAT RELATED TO THE RETURN?)\n",
    "\n",
    "\n",
    "##########################################\n",
    "###FIGURE OUT THE LOSS FUNCTION\n",
    "\n",
    "## WHY DO YOU PAD THE INPUT LIKE THAT\n",
    "\n",
    "## WHY DO YOU SLICE THE INPUT\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:46.624860Z",
     "start_time": "2018-09-22T11:58:46.622855Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRY TO TRAIN IT, WRITE OWN TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:25.249402Z",
     "start_time": "2018-09-22T11:58:25.247405Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONVERT IT TO BIDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:08.624959Z",
     "start_time": "2018-09-22T11:58:08.622955Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEACHER FORCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:15.176179Z",
     "start_time": "2018-09-22T11:58:15.174175Z"
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:58:34.922938Z",
     "start_time": "2018-09-22T11:58:34.920934Z"
    }
   },
   "outputs": [],
   "source": [
    "# ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:59:06.256912Z",
     "start_time": "2018-09-22T11:59:06.254906Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONVERT MODEL TO PRDOCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
